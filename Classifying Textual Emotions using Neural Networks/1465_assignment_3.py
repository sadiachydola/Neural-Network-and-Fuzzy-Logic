# -*- coding: utf-8 -*-
"""1465_Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KdcWsEovP_bDNhUa-EuR__Sh5lGc8--O
"""

# TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community
import tensorflow as tf

# Embedding Layer, it is a vector and it have input dimension(1000) and output dimension(5)
from tensorflow.keras import layers

embedding_layer = layers.Embedding(1000, 5)

# NumPy is used to work with arrays. The array object in NumPy is called ndarray.
from numpy import array
import tensorflow as tf
# A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models.
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
# A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.
from tensorflow.keras.models import  Sequential
# LSTM - Long Short-Term Memory layer
from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional

# Uploading the train dataset
from google.colab import files

uploaded = files.upload()

# Uploading the dev dataset
from google.colab import files

uploaded = files.upload()

# Declared two lists train_data and test_data and also reading mode
train_data = []
test_data = []

with open('train.txt', 'r') as train_file:
    for line in train_file:
        train_data.append(line)

with open('dev.txt', 'r') as test_file:
    for line in test_file:
        test_data.append(line)

# Separating labels and examples (documents) from train data and test (dev) data 
train_documents = []
train_labels = []
test_documents = []
test_labels = []

for line in train_data:
    """ Spliting the whole document in each line when it gets first white space """
    splitted_line = line.split(' ',1)
    # Separate the labels and examples (documents) in different list
    train_labels.append(splitted_line[0])
    train_documents.append(splitted_line[1])
# print(splitted_line)

for line in test_data:
    """ Spliting the whole document in each line when it gets first white space"""
    splitted_line = line.split(' ',1)
    # Separate the labels and examples (documents) in different list
    test_labels.append(splitted_line[0])
    test_documents.append(splitted_line[1])
# print(splitted_line)
# print(train_labels)
# print(train_documents)
# print(test_labels)
# print(test_documents)

# Declaring a dictionary where the labels into integer numbers which start from (oth position to 5th position)
label_dictionary = {'"বিস্ময়"':0,
                    '"ভয়"':1,
                    '"রাগ"':2,
                    '"বিতৃষ্ণা"':3,
                    '"বিষণ্ণতা"':4,
                    '"সুখী"':5}
# label_dictionary['রাগ']

# Tokenization and Converting Words into Sequences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_documents)
dense_train_doc = tokenizer.texts_to_sequences(train_documents)
dense_test_doc = tokenizer.texts_to_sequences(test_documents)

# Padding the Training Documents in order to make them equal length
# MAX_LENGTH = len(max(dense_train_doc, key=len))
MAX_LENGTH = max([len(i) for i in dense_train_doc])

# Padding the train documents
padded_train_doc = pad_sequences(dense_train_doc, maxlen=MAX_LENGTH, padding='post')
print(padded_train_doc)
# Padding the test documents
padded_test_doc = pad_sequences(dense_test_doc, maxlen=MAX_LENGTH, padding='post')
print(padded_test_doc)

# Model Declaration
VOCUB_SIZE = max([max(i) for i in dense_train_doc])+1
model = Sequential()

# Embedding Layer where input dimension is assigned as vocubulary size, and length is equal of maximum length in dense_train_doc
embedding_layer = Embedding(input_dim=VOCUB_SIZE, output_dim=20, input_length=MAX_LENGTH)
model.add(embedding_layer)

# LSTM - for better performance,NLP
# Bidirectional LSTM [Extra] Return sequence false because we don't want to print it
forward_layers = LSTM(units=128, return_sequences=False)
backward_layers = LSTM(units=128, return_sequences=False, go_backwards=True)
model.add(Bidirectional(layer=forward_layers, backward_layer=backward_layers))

# Output layer added softmax as activation function
model.add(Dense(units=6, activation='softmax'))

# Using Nadam optimizer and CategoricalCrossentropy as loss function to compile the model
model.compile(optimizer='Nadam', loss='categorical_crossentropy', metrics=['acc'])

print(model.summary())

import numpy as np
# Convert the levels into Binary matrix

train_lbls = [label_dictionary[i] for i in train_labels]
print(train_lbls)
train_labels = tf.keras.utils.to_categorical(train_lbls)
print(train_labels)

test_lbls = [label_dictionary[i] for i in test_labels]
# print(test_lbls)
test_labels = tf.keras.utils.to_categorical(test_lbls)
print(test_labels)

model.fit(padded_train_doc, train_labels, epochs=1000, verbose=1)

padded_test_doc = pad_sequences(dense_test_doc, maxlen=MAX_LENGTH, padding='post')
print(padded_test_doc)

class_type = {0:'"বিস্ময়"',
              1:'"ভয়"',
              2:'"রাগ"',
              3:'"বিতৃষ্ণা"',
              4:'"বিষণ্ণতা"',
              5:'"সুখী"'}
ynew = np.argmax(model.predict(padded_test_doc), axis=-1)
print(ynew)
for indx, doc in enumerate(test_documents):
  print("Test Doc - {}: Prediction = {} ({}) -- {}".format(indx, ynew[indx], class_type[ynew[indx]], doc))